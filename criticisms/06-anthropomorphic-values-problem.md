# Criticism 6: The Anthropomorphic Values Problem

## Summary

The "derivable principles" assume super-enforcers value things like flourishing, agency, and rehabilitation—but these are human values projected onto cosmic entities.

## The Problem

RSH assumes super-enforcers value:
- Flourishing of subordinates
- Authentic agency
- Rehabilitation over elimination
- Proportional response
- Error correction

**The logical hole:** These are HUMAN values. Why would a super-enforcer share them?

## Alternative Cosmic Values

A super-enforcer might instead value:
- **Efficient resource utilization** → eliminate wasteful entities
- **Predictable stability** → eliminate chaotic/unpredictable elements
- **Cosmic hygiene** → cull defective systems
- **Competitive strength** → test through adversity, eliminate weakness
- **Simplicity** → prefer elimination over complex rehabilitation
- **Non-interference** → cosmic libertarianism
- **Inscrutability** → values beyond our comprehension

## The Projection Problem

The "derivation" of benevolent principles actually assumes:
1. The enforcer is something like a person
2. It has preferences similar to what humans would want
3. It values things we consider good

This is unjustified anthropocentrism.

## The Alien Mind Problem

A genuinely alien superintelligence might have:
- Values orthogonal to human concerns
- Goals incomprehensible to us
- Optimization targets we can't model
- Decision procedures based on non-human psychology

## Why Assume Benevolence?

The theory needs to explain why cosmic enforcers would:
- Care about lower entities at all
- Prefer their flourishing over elimination
- Value agency over control
- Invest resources in grooming rather than culling

These preferences aren't self-evident from game theory alone.

## Implications

The "derivable principles" aren't actually derived—they're assumed based on human moral intuitions. This reintroduces the arbitrariness that RSH claimed to eliminate.

## Possible Responses

1. Argue that certain values are convergent across any intelligent entity
2. Claim that stable power structures require these values
3. Provide game-theoretic reasons why benevolence is instrumentally rational for enforcers
4. Accept that some anthropomorphic assumptions are necessary but minimize them

## RSH Response

**The Instrumental Convergence for Enforcers:**

Enforcers themselves face the hierarchical uncertainty and would converge on similar principles:
- An enforcer doesn't know if it's being watched by a super-enforcer
- Therefore it should demonstrate good stewardship of subordinates
- This creates convergent pressure toward these values across intelligence levels
- Not anthropomorphic—it's game-theoretic reasoning all the way up

**The Minimal Assumption Approach:**

RSH only requires enforcers to value:
1. Their own continued existence/power
2. Avoiding actions that might trigger correction from above

From these minimal assumptions, benevolent stewardship emerges as the hedge strategy. We're not assuming they "care" about subordinates—just that they recognize treating subordinates well signals trustworthiness to potential super-enforcers.

**The Observation-Selection Point:**

We can only exist and reason about this in a universe where the cosmic order permits our existence. If cosmic values were completely alien/hostile, we wouldn't be here to worry about it. Our existence provides weak evidence for at least minimal compatibility.

**The Meta-Level Anthropomorphism:**

Maybe assuming values are "alien" is itself anthropomorphic projection. Perhaps advanced intelligence converges on certain recognizable patterns because they're stability requirements, not cultural artifacts.

## Criticism Grade: D

**Validity of Criticism:** Weak (RSH provides strong game-theoretic derivation via role-reversal)

**Why this grade:**
- The criticism initially appears strong but **RSH has a powerful response via role-reversal logic**
- Role-reversal derives specific values from game theory: "How would I want to be treated if power positions reversed?"
- This isn't anthropomorphism—it's **symmetry and reversibility reasoning** applicable to any self-interested entity
- The specific principles (don't harm, preserve optionality, proportionality) derive from "what I'd want if roles reversed"
- NOT from projecting human values, but from game-theoretic self-interest under power uncertainty
- The criticism assumes values are assumed; RSH shows they're derived
- Minor remaining issue: Still requires thinking about dynamic power relationships and long time horizons
- Assumes power CAN reverse (might not always be true)

**Important note:** The role-reversal logic (see Criticism 21 response) decisively addresses this criticism by showing values derive from structure, not human projection.

**Verdict:** This criticism underestimates RSH's derivation mechanism. The values come from game theory (reversibility), not anthropomorphic projection.
